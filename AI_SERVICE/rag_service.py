
import os
import httpx
from typing import List, Dict, Optional
import google.generativeai as genai
import PIL.Image
import io
import base64
"""
RAG Service - Retrieval-Augmented Generation cho Phonify AI Chat

CHI·∫æN L∆Ø·ª¢C RAG:
===============
1. Vector Search (Semantic Search) - CH√çNH
   - D√πng sentence-transformers ƒë·ªÉ t·∫°o embeddings
   - Vector similarity search thay v√¨ keyword matching
   - Hi·ªÉu ng·ªØ nghƒ©a: "m√°y ch·ª•p h√¨nh xuy√™n m√†n ƒë√™m" ‚Üí t√¨m ƒë√∫ng s·∫£n ph·∫©m c√≥ t√≠nh nƒÉng ƒë√≥
   - In-memory vector store v·ªõi caching

2. Pipeline:
   a) Vector Embedding: T·∫°o embeddings cho query v√† products
      - Query embedding: T·ª´ c√¢u h·ªèi c·ªßa user
      - Product embeddings: Cache trong memory, update khi c·∫ßn
   
   b) Vector Similarity Search: Cosine similarity gi·ªØa query v√† products
      - Top-K retrieval (l·∫•y top 10-20 s·∫£n ph·∫©m li√™n quan nh·∫•t)
   
   c) Optional LLM Reranking: C√≥ th·ªÉ d√πng th√™m ƒë·ªÉ fine-tune ranking
   
   d) Multi-source Retrieval: Products + Reviews + FAQs

3. ∆Øu ƒëi·ªÉm Vector Search:
   - Hi·ªÉu ng·ªØ nghƒ©a, kh√¥ng ph·ª• thu·ªôc keyword matching
   - X·ª≠ l√Ω ƒë∆∞·ª£c synonyms, paraphrasing
   - Kh√¥ng c·∫ßn extract_search_term() n·ªØa
   - Ch√≠nh x√°c h∆°n cho c√¢u h·ªèi ph·ª©c t·∫°p

4. Trade-offs:
   - C·∫ßn compute embeddings (nh∆∞ng c√≥ cache)
   - T·ªën memory ƒë·ªÉ store vectors (nh∆∞ng in-memory ƒë·ªß cho h√†ng ngh√¨n products)
   - C√≥ th·ªÉ k·∫øt h·ª£p v·ªõi keyword search (hybrid) n·∫øu c·∫ßn
"""

# Fix encoding for Vietnamese characters on Windows
import sys
import io
if sys.platform == 'win32':
    # Set UTF-8 encoding for stdout/stderr on Windows
    if not isinstance(sys.stdout, io.TextIOWrapper):
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    if not isinstance(sys.stderr, io.TextIOWrapper):
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

import os
import httpx
from typing import List, Dict, Optional, Tuple
import google.generativeai as genai
import numpy as np
from sentence_transformers import SentenceTransformer
import re
import PyPDF2 # Th∆∞ vi·ªán m·ªõi ƒë·ªÉ ƒë·ªçc PDF
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-flash-latest")
BACKEND_URL = os.getenv("BACKEND_URL", "http://localhost:8000")

genai.configure(api_key=GEMINI_API_KEY)



# Helper function for safe printing Vietnamese text
def safe_print(*args, **kwargs):
    """Print with UTF-8 encoding, handles Vietnamese characters safely"""
    try:
        print(*args, **kwargs)
    except UnicodeEncodeError:
        # Fallback: encode to ASCII with replacement
        safe_args = [str(arg).encode('ascii', 'replace').decode('ascii') if isinstance(arg, str) else arg for arg in args]
        print(*safe_args, **kwargs)

# Vector Search Setup
# D√πng model h·ªó tr·ª£ ti·∫øng Vi·ªát t·ªët
# Fallback: paraphrase-multilingual-MiniLM-L12-v2 (h·ªó tr·ª£ 50+ ng√¥n ng·ªØ bao g·ªìm ti·∫øng Vi·ªát)
EMBEDDING_MODEL_NAME = "paraphrase-multilingual-MiniLM-L12-v2"
_embedding_model = None
_product_embeddings_cache = {}  # {product_id: embedding_vector}
_product_metadata_cache = {}  # {product_id: product_dict}
_policy_database = []         # L∆∞u c√°c ƒëo·∫°n vƒÉn b·∫£n t·ª´ PDF
_policy_embeddings_cache = [] # L∆∞u vector t∆∞∆°ng ·ª©ng c·ªßa c√°c ƒëo·∫°n ƒë√≥
_model_loading_started = False
_model_loading_error = None

def get_embedding_model():
    """Lazy load embedding model (ch·ªâ load 1 l·∫ßn khi c·∫ßn)"""
    global _embedding_model, _model_loading_started, _model_loading_error
    if _embedding_model is None and not _model_loading_started:
        _model_loading_started = True
        print("[RAG] Loading embedding model (this may take 1-2 minutes on first run)...")
        print("[RAG] If download fails, system will fallback to keyword search")
        try:
            # Set environment variable ƒë·ªÉ tƒÉng timeout cho HuggingFace
            import os
            os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '300'  # 5 ph√∫t
            
            _embedding_model = SentenceTransformer(
                EMBEDDING_MODEL_NAME,
                device='cpu'  # D√πng CPU ƒë·ªÉ tr√°nh l·ªói GPU
            )
            print(f"[RAG] ‚úÖ Loaded embedding model: {EMBEDDING_MODEL_NAME}")
            _model_loading_error = None
        except Exception as e:
            _model_loading_error = str(e)
            print(f"[RAG] ‚ùå Failed to load embedding model: {e}")
            print(f"[RAG] System will use keyword search as fallback")
            # Kh√¥ng raise error, ƒë·ªÉ h·ªá th·ªëng fallback v·ªÅ keyword search
            _model_loading_started = False  # Cho ph√©p retry sau
    elif _embedding_model is None and _model_loading_started and _model_loading_error is None:
        # Model ƒëang ƒë∆∞·ª£c load, ƒë·ª£i m·ªôt ch√∫t
        import time
        max_wait = 10  # ƒê·ª£i t·ªëi ƒëa 10 gi√¢y
        waited = 0
        while _embedding_model is None and waited < max_wait and _model_loading_error is None:
            time.sleep(0.5)
            waited += 0.5
    return _embedding_model

def get_embedding_model_status():
    """L·∫•y tr·∫°ng th√°i c·ªßa embedding model"""
    global _embedding_model, _model_loading_started, _model_loading_error
    if _embedding_model is not None:
        return {"status": "loaded", "model": EMBEDDING_MODEL_NAME}
    elif _model_loading_error:
        return {"status": "error", "error": _model_loading_error}
    elif _model_loading_started:
        return {"status": "loading", "model": EMBEDDING_MODEL_NAME}
    else:
        return {"status": "not_started", "model": EMBEDDING_MODEL_NAME}

# Disable pre-load trong background thread v√¨ c√≥ th·ªÉ g√¢y timeout
# Model s·∫Ω ƒë∆∞·ª£c load khi c·∫ßn (lazy load) v√† fallback v·ªÅ keyword search n·∫øu fail
# Pre-load c√≥ th·ªÉ ƒë∆∞·ª£c enable l·∫°i sau khi model ƒë√£ ƒë∆∞·ª£c download th√†nh c√¥ng

def generate_embedding(text: str) -> np.ndarray:
    """T·∫°o embedding vector cho m·ªôt ƒëo·∫°n text"""
    if not text or not text.strip():
        text = ""
    model = get_embedding_model()
    embedding = model.encode(text, normalize_embeddings=True)
    return embedding

def generate_product_embedding(product: Dict) -> np.ndarray:
    """T·∫°o embedding cho m·ªôt s·∫£n ph·∫©m t·ª´ c√°c th√¥ng tin: name, category, description"""
    text_parts = []
    
    # T√™n s·∫£n ph·∫©m (quan tr·ªçng nh·∫•t)
    if product.get("name"):
        text_parts.append(product["name"])
    
    # Category
    if product.get("category"):
        text_parts.append(product["category"])
    
    # Description
    if product.get("description"):
        desc = product["description"]
        # Gi·ªõi h·∫°n description ƒë·ªÉ tr√°nh qu√° d√†i
        if len(desc) > 500:
            desc = desc[:500]
        text_parts.append(desc)
    
    # Brand (n·∫øu c√≥ trong name)
    # C√°c t√≠nh nƒÉng ƒë·∫∑c tr∆∞ng c√≥ th·ªÉ extract t·ª´ description
    
    combined_text = " ".join(text_parts)
    return generate_embedding(combined_text)

def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """T√≠nh cosine similarity gi·ªØa 2 vectors"""
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

async def vector_search_products(
    query: str, 
    products: List[Dict], 
    top_k: int = 10
) -> List[Tuple[Dict, float]]:
    """
    Vector similarity search: T√¨m top-k s·∫£n ph·∫©m li√™n quan nh·∫•t v·ªõi query.
    
    Returns:
        List of (product, similarity_score) tuples, sorted by similarity descending
    """
    if not products:
        return []
    
    try:
        # 1. T·∫°o embedding cho query
        query_embedding = generate_embedding(query)
        print(f"üî¢ [Vector Search] Generated query embedding (dim={len(query_embedding)})")
        
        # 2. T·∫°o/cache embeddings cho products
        product_scores = []
        for product in products:
            product_id = str(product.get("productId", id(product)))
            
            # Check cache
            if product_id in _product_embeddings_cache:
                product_embedding = _product_embeddings_cache[product_id]
            else:
                # Generate v√† cache
                product_embedding = generate_product_embedding(product)
                _product_embeddings_cache[product_id] = product_embedding
                _product_metadata_cache[product_id] = product
            
            # 3. T√≠nh similarity
            similarity = cosine_similarity(query_embedding, product_embedding)
            product_scores.append((product, float(similarity)))
        
        # 4. Sort by similarity (descending) v√† l·∫•y top-k
        product_scores.sort(key=lambda x: x[1], reverse=True)
        top_results = product_scores[:top_k]
        
        # Log similarity scores ƒë·ªÉ debug
        if top_results:
            top_similarity = top_results[0][1]
            print(f"üéØ [Vector Search] Found {len(top_results)} products")
            print(f"   Top similarity scores: {[f'{score:.3f}' for _, score in top_results[:5]]}")
        else:
            print(f"üéØ [Vector Search] No products found")
        
        return top_results
    
    except Exception as e:
        print(f"[RAG] Vector search failed: {e}")
        # Fallback: tr·∫£ v·ªÅ products g·ªëc
        return [(p, 0.0) for p in products[:top_k]]

def should_search_policies(message: str) -> bool:
    keywords = [
        "ch√≠nh s√°ch", "b·∫£o h√†nh", "ƒë·ªïi tr·∫£", "giao h√†ng", 
        "v√†o n∆∞·ªõc", "r∆°i v·ª°", "h·ªèng", "s·ª≠a", "chi ph√≠", "m√°y b·ªã" 
    ]
    return any(k in message.lower() for k in keywords)

def should_search_products(message: str) -> bool:
    lower_message = message.lower().strip()
    
    if len(lower_message) < 2:
        return False
    
    keywords = [
        "s·∫£n ph·∫©m", "ƒëi·ªán tho·∫°i", "phone", "iphone", "samsung", "xiaomi", "oppo", "vivo",
        "gi√°", "gi√° bao nhi√™u", "t·ªìn kho", "c√≤n h√†ng", "h·∫øt h√†ng", "mua", "b√°n",
        "t√¨m", "c√≥", "n√†o", "lo·∫°i", "d√≤ng", "m·∫´u", "model", "shop", "c·ª≠a h√†ng",
        "tri·ªáu", "tr", "ngh√¨n", "k", "vnd", "ƒë·ªìng", "sp", "h√†ng", "gi·ªõi thi·ªáu", "t∆∞ v·∫•n",
        "galaxy", "pixel", "google",
        # TH√äM T·ª™ KH√ìA M√î T·∫¢ NHANH
        "m√¥ t·∫£", "t√≥m t·∫Øt", "review", "ƒë√°ng mua", "chi ti·∫øt", "th√¥ng s·ªë",
    ]
    
    return any(keyword in lower_message for keyword in keywords)

def extract_search_term(message: str) -> str:
    """
    Tr√≠ch xu·∫•t t·ª´ kh√≥a t√¨m ki·∫øm t·ª´ c√¢u h·ªèi c·ªßa user.
    ∆Øu ti√™n: Brand > T√≠nh nƒÉng ƒë·∫∑c tr∆∞ng > T·ª´ kh√≥a chung
    """
    lower_message = message.lower().strip()
    brand_keywords = ["iphone", "samsung", "xiaomi", "oppo", "vivo", "realme", "oneplus", "nokia", "huawei", "galaxy", "pixel", "google"]
    
    # ∆Øu ti√™n 1: T√¨m brand
    for brand in brand_keywords:
        if brand in lower_message:
            return brand
    
    # ∆Øu ti√™n 2: T√¨m t·ª´ kh√≥a ƒë·∫∑c tr∆∞ng (t√≠nh nƒÉng, model)
    feature_keywords = [
        "ch·ª•p h√¨nh", "ch·ª•p ·∫£nh", "camera", "pin", "m√†n h√¨nh", "ram", "rom", 
        "xuy√™n m√†n", "night mode", "zoom", "selfie", "5g", "4g",
        "pro", "max", "ultra", "plus", "mini", "se"
    ]
    
    for feature in feature_keywords:
        if feature in lower_message:
            # L·∫•y c·∫£ c·ª•m t·ª´ n·∫øu c√≥
            idx = lower_message.find(feature)
            words_around = lower_message[max(0, idx-10):idx+len(feature)+10].split()
            # L·ªçc v√† l·∫•y c√°c t·ª´ c√≥ nghƒ©a
            meaningful_words = [w for w in words_around if len(w) > 2 and w not in ["c√≥", "l√†", "v√†", "v·ªõi"]]
            if meaningful_words:
                return " ".join(meaningful_words[:3])  # L·∫•y t·ªëi ƒëa 3 t·ª´
    
    # ∆Øu ti√™n 3: N·∫øu c√≥ "ƒëi·ªán tho·∫°i" nh∆∞ng kh√¥ng c√≥ brand/t√≠nh nƒÉng c·ª• th·ªÉ ‚Üí tr·∫£ r·ªóng ƒë·ªÉ search r·ªông
    if "ƒëi·ªán tho·∫°i" in lower_message or "phone" in lower_message:
        return ""
    
    # ∆Øu ti√™n 4: L·∫•y c√°c t·ª´ c√≥ nghƒ©a (b·ªè stop words)
    stop_words = ["c√≥", "kh√¥ng", "l√†", "c·ªßa", "v√†", "v·ªõi", "cho", "t·ª´", "ƒë·∫øn", "v·ªÅ", "n√†o", "g√¨", "m√°y"]
    words = [w for w in lower_message.split() if len(w) > 2 and w not in stop_words]
    if words:
        # L·∫•y t·ªëi ƒëa 2-3 t·ª´ ƒë·∫ßu ti√™n c√≥ nghƒ©a
        return " ".join(words[:3])
    
    return ""

def extract_price_intent(message: str) -> Tuple[str, int]:
    """
    Tr√≠ch xu·∫•t ƒëi·ªÅu ki·ªán gi√° v√† gi√° m·ª•c ti√™u (VNƒê) t·ª´ c√¢u h·ªèi.
    Return: (price_condition, price_value_vnd)
      - price_condition: "duoi" | "tu" | "tren" | "khoang" | ""
      - price_value_vnd: int (0 n·∫øu kh√¥ng c√≥)
    """
    text = (message or "").lower().strip()
    if not text:
        return "", 0

    condition = ""
    if "d∆∞·ªõi" in text or "duoi" in text:
        condition = "duoi"
    elif "tr√™n" in text or "tren" in text:
        condition = "tren"
    elif "t·ª´" in text or re.search(r"\btu\b", text):
        condition = "tu"
    elif "kho·∫£ng" in text or "khoang" in text or "t·∫ßm" in text or re.search(r"\btam\b", text):
        condition = "khoang"

    # Match s·ªë + ƒë∆°n v·ªã (h·ªó tr·ª£ 8, 8.5, 8,5 tri·ªáu)
    m = re.search(r"(\d+(?:[.,]\d+)?)\s*(tri·ªáu|tr|k|ngh√¨n|ng√†n|vnƒë|vnd|ƒë|ƒë·ªìng|dong)?", text)
    if not m:
        return condition, 0

    raw_amount, unit = m.group(1), (m.group(2) or "").strip()
    try:
        amount = float(raw_amount.replace(",", "."))
    except Exception:
        return condition, 0

    unit = unit.lower()
    if unit in ["tri·ªáu", "tr"]:
        value = int(amount * 1_000_000)
    elif unit in ["k", "ngh√¨n", "ng√†n"]:
        value = int(amount * 1_000)
    elif unit in ["vnƒë", "vnd", "ƒë", "ƒë·ªìng", "dong"]:
        value = int(amount)
    else:
        # Kh√¥ng c√≥ ƒë∆°n v·ªã: heuristic
        # N·∫øu s·ªë nh·ªè v√† c√¢u c√≥ "tri·ªáu/tr/t·∫ßm/kho·∫£ng" th√¨ hi·ªÉu l√† tri·ªáu
        if amount < 1000 and ("tri·ªáu" in text or re.search(r"\btr\b", text) or "t·∫ßm" in text or "kho·∫£ng" in text or "khoang" in text):
            value = int(amount * 1_000_000)
        else:
            value = int(amount)

    if value < 0:
        value = 0

    # N·∫øu c√≥ gi√° m√† kh√¥ng c√≥ ƒëi·ªÅu ki·ªán, m·∫∑c ƒë·ªãnh coi l√† "khoang"
    if value and not condition:
        condition = "khoang"

    return condition, value

def prefilter_products_by_price(products: List[Dict], price_condition: str, price_value: int) -> List[Dict]:
    """
    L·ªçc s·∫£n ph·∫©m theo t·∫ßm gi√° tr∆∞·ªõc khi vector search ƒë·ªÉ tr√°nh l·ªách gi√°.
    Quy ∆∞·ªõc:
    - khoang/t·∫ßm: +/-30%
    - duoi: [70%..100%] * target
    - tren/tu: [100%..130%] * target
    N·∫øu l·ªçc ra r·ªóng -> tr·∫£ list g·ªëc (kh√¥ng l√†m m·∫•t d·ªØ li·ªáu).
    """
    if not products or not price_value:
        return products

    def get_price(p: Dict) -> int:
        try:
            return int(p.get("salePrice") or p.get("price") or p.get("minPrice") or 0)
        except Exception:
            return 0

    if price_condition == "duoi":
        min_p = int(price_value * 0.7)
        max_p = int(price_value)
    elif price_condition in ["tren", "tu"]:
        min_p = int(price_value)
        max_p = int(price_value * 1.3)
    else:  # khoang/unknown
        min_p = int(price_value * 0.7)
        max_p = int(price_value * 1.3)

    filtered = [p for p in products if (min_p <= get_price(p) <= max_p)]
    if filtered:
        safe_print(f"üéØ [RAG] Price prefilter kept {len(filtered)}/{len(products)} products in [{min_p:,}..{max_p:,}]")
        return filtered

    safe_print(f"üéØ [RAG] Price prefilter removed all products for [{min_p:,}..{max_p:,}], keeping original list")
    return products

async def extract_search_term_with_llm(message: str) -> str:
    """
    D√πng LLM ƒë·ªÉ tr√≠ch xu·∫•t t·ª´ kh√≥a t√¨m ki·∫øm ch√≠nh x√°c h∆°n.
    H·ªØu √≠ch cho c√°c c√¢u h·ªèi ph·ª©c t·∫°p nh∆∞ "m√°y ch·ª•p h√¨nh xuy√™n m√†n ƒë√™m".
    
    Trade-off: T·ªën 1 API call nh∆∞ng tƒÉng ƒë·ªô ch√≠nh x√°c ƒë√°ng k·ªÉ.
    """
    try:
        model_name = GEMINI_MODEL
        if not model_name.startswith("models/"):
            model_name = f"models/{model_name}"
        try:
            model = genai.GenerativeModel(model_name=model_name)
        except TypeError:
            model = genai.GenerativeModel(GEMINI_MODEL)
        
        prompt = f"""B·∫°n l√† h·ªá th·ªëng tr√≠ch xu·∫•t t·ª´ kh√≥a t√¨m ki·∫øm. T·ª´ c√¢u h·ªèi c·ªßa kh√°ch h√†ng, h√£y tr√≠ch xu·∫•t 1-3 t·ª´ kh√≥a quan tr·ªçng nh·∫•t ƒë·ªÉ t√¨m s·∫£n ph·∫©m ƒëi·ªán tho·∫°i.

C√¢u h·ªèi: "{message}"

Y√™u c·∫ßu:
- N·∫øu c√≥ t√™n th∆∞∆°ng hi·ªáu (iPhone, Samsung, Xiaomi...), ∆∞u ti√™n l·∫•y t√™n th∆∞∆°ng hi·ªáu
- N·∫øu c√≥ t√≠nh nƒÉng ƒë·∫∑c tr∆∞ng (ch·ª•p h√¨nh xuy√™n m√†n, camera zoom, pin l√¢u...), l·∫•y t√≠nh nƒÉng ƒë√≥
- N·∫øu c√≥ model c·ª• th·ªÉ (iPhone 15, Galaxy S24...), l·∫•y model
- Ch·ªâ tr·∫£ v·ªÅ t·ª´ kh√≥a, kh√¥ng gi·∫£i th√≠ch, t·ªëi ƒëa 3 t·ª´, c√°ch nhau b·ªüi d·∫•u c√°ch

V√≠ d·ª•:
- "m√°y ch·ª•p h√¨nh xuy√™n m√†n ƒë√™m" ‚Üí "ch·ª•p h√¨nh xuy√™n m√†n"
- "ƒëi·ªán tho·∫°i iPhone gi√° r·∫ª" ‚Üí "iphone"
- "Samsung Galaxy S24 c√≥ camera t·ªët kh√¥ng" ‚Üí "samsung galaxy s24"
- "ƒëi·ªán tho·∫°i pin l√¢u" ‚Üí "pin l√¢u"

T·ª´ kh√≥a:"""
        
        response = model.generate_content(prompt)
        search_term = response.text.strip().lower()
        
        # L√†m s·∫°ch k·∫øt qu·∫£ (b·ªè d·∫•u c√¢u, gi·ªØ l·∫°i t·ª´ kh√≥a)
        search_term = " ".join([w for w in search_term.split() if len(w) > 1])
        return search_term[:50]  # Gi·ªõi h·∫°n ƒë·ªô d√†i
    except Exception as e:
        print(f"[RAG] LLM search term extraction failed: {e}, falling back to rule-based")
        return extract_search_term(message)

def extract_keywords(query: str) -> List[str]:
    stop_words = ["c√≥", "kh√¥ng", "l√†", "c·ªßa", "v√†", "v·ªõi", "cho", "t·ª´", "ƒë·∫øn", "v·ªÅ", "n√†o", "g√¨"]
    return [w for w in query.lower().split() if len(w) > 2 and w not in stop_words][:5]


async def get_products_from_backend(backend_url: str, search_term: str = "", limit: int = 50) -> List[Dict]:
    """
    L·∫•y products t·ª´ backend.

    Args:
        search_term: T·ª´ kh√≥a t√¨m ki·∫øm (optional, c√≥ th·ªÉ d√πng cho keyword fallback)
        limit: Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng products (ƒë·ªÉ vector search kh√¥ng qu√° ch·∫≠m)
    """
    print(f"[RAG] get_products_from_backend called with search_term='{search_term}', limit={limit}")
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            # N·∫øu c√≥ search_term, d√πng keyword search (fallback)
            # N·∫øu kh√¥ng, l·∫•y t·∫•t c·∫£ products ƒë·ªÉ vector search
            params = {}
            if search_term:
                params["search"] = search_term
            if limit:
                params["limit"] = limit
            
            response = await client.get(
                f"{backend_url}/api/v1/internal/products/search",
                params=params if params else {}
            )
            if response.status_code == 200:
                data = response.json()
                print(f"[RAG] Backend response status: {response.status_code}")
                # H·ªó tr·ª£ c·∫£ hai format: {data: {products: [...]}} v√† {products: [...]}
                products = (
                    data.get("data", {}).get("products")
                    if isinstance(data, dict) else None
                )
                if not products and isinstance(data, dict):
                    products = data.get("products")

                return products or []

                print(f"[RAG] Retrieved {len(products or [])} products from backend")
                if products and len(products) > 0:
                    print(f"[RAG] First product: {products[0].get('name', 'Unknown')}")
                return products or []
            print(f"[RAG] Backend error: {response.status_code}")
            return []
    except Exception as e:
        print(f"[RAG] Error fetching products: {e}")
        return []

async def get_reviews_from_backend(backend_url: str, keywords: List[str]) -> List[Dict]:
    if not keywords:
        return []


    
    try:
        search_query = " ".join(keywords)
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(
                f"{backend_url}/api/v1/internal/reviews/search",
                params={"search": search_query}
            )
            if response.status_code == 200:
                data = response.json()


                # H·ªó tr·ª£ c·∫£ hai format: {data: {reviews: [...]}} v√† {reviews: [...]}
                reviews = (
                    data.get("data", {}).get("reviews")
                    if isinstance(data, dict) else None
                )
                if not reviews and isinstance(data, dict):
                    reviews = data.get("reviews")
                return reviews[:5]
            return []
    except Exception as e:
        print(f"[RAG] Error fetching reviews: {e}")
        return []

def get_faqs(query: str) -> List[Dict]:
    faq_database = [

        {"question": "C√≥ ch√≠nh s√°ch ƒë·ªïi tr·∫£ kh√¥ng?", "answer": "Ch√∫ng t√¥i c√≥ ch√≠nh s√°ch ƒë·ªïi tr·∫£ trong v√≤ng 7 ng√†y n·∫øu s·∫£n ph·∫©m c√≤n nguy√™n seal, kh√¥ng tr·∫ßy x∆∞·ªõc."},
        {"question": "C√≥ b·∫£o h√†nh kh√¥ng?", "answer": "T·∫•t c·∫£ s·∫£n ph·∫©m ƒë·ªÅu c√≥ b·∫£o h√†nh ch√≠nh h√£ng t·ª´ 12-24 th√°ng t√πy s·∫£n ph·∫©m."},
        {"question": "C√≥ ship COD kh√¥ng?", "answer": "C√≥, ch√∫ng t√¥i h·ªó tr·ª£ thanh to√°n khi nh·∫≠n h√†ng (COD) tr√™n to√†n qu·ªëc."},
        {"question": "Th·ªùi gian giao h√†ng?", "answer": "Th·ªùi gian giao h√†ng t·ª´ 1-3 ng√†y l√†m vi·ªác t√πy khu v·ª±c."}
    ]
    query_lower = query.lower()
    return [faq for faq in faq_database if query_lower in faq["question"].lower() or query_lower in faq["answer"].lower()][:3]

async def semantic_search(query: str, products: List[Dict]) -> List[Dict]:
    try:
        model_name = GEMINI_MODEL
        if not model_name.startswith("models/"):
            model_name = f"models/{model_name}"
        try:
            model = genai.GenerativeModel(model_name=model_name)
        except TypeError:
            model = genai.GenerativeModel(GEMINI_MODEL)
        
        product_list = "\n".join([
            f"{idx + 1}. {p['name']} - {p.get('category', '')} - {p.get('description', 'Kh√¥ng c√≥ m√¥ t·∫£')}"
            for idx, p in enumerate(products)
        ])
        
        prompt = f"""B·∫°n l√† m·ªôt h·ªá th·ªëng t√¨m ki·∫øm th√¥ng minh. H√£y ph√¢n t√≠ch c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng v√† x·∫øp h·∫°ng c√°c s·∫£n ph·∫©m sau theo m·ª©c ƒë·ªô li√™n quan.

C√¢u h·ªèi: "{query}"

Danh s√°ch s·∫£n ph·∫©m:
{product_list}

H√£y tr·∫£ v·ªÅ danh s√°ch s·ªë th·ª© t·ª± (1, 2, 3...) c·ªßa c√°c s·∫£n ph·∫©m ƒë∆∞·ª£c x·∫øp h·∫°ng t·ª´ cao xu·ªëng th·∫•p theo m·ª©c ƒë·ªô li√™n quan, c√°ch nhau b·ªüi d·∫•u ph·∫©y. Ch·ªâ tr·∫£ v·ªÅ s·ªë, kh√¥ng gi·∫£i th√≠ch.

V√≠ d·ª•: 3, 1, 5, 2, 4"""
        
        response = model.generate_content(prompt)
        text = response.text.strip()
        
        ranked_indices = [
            int(s.strip()) - 1 
            for s in text.split(",") 
            if s.strip().isdigit() and 0 <= int(s.strip()) - 1 < len(products)
        ]
        
        ranked_products = []
        used_indices = set()
        
        for idx in ranked_indices:
            if idx not in used_indices:
                ranked_products.append(products[idx])
                used_indices.add(idx)

        for idx, product in enumerate(products):
            if idx not in used_indices:
                ranked_products.append(product)
        
        return ranked_products
    except Exception as e:
        print(f"[RAG] Semantic search failed: {e}")
        return products

# --- B·∫ÆT ƒê·∫¶U PH·∫¶N T√çCH H·ª¢P PDF ---
def load_policies_from_pdfs(folder_path="./data/policies"):
    """Qu√©t th∆∞ m·ª•c v√† tr√≠ch xu·∫•t text t·ª´ file PDF ch√≠nh s√°ch"""
    global _policy_database, _policy_embeddings_cache
    if not os.path.exists(folder_path):
        os.makedirs(folder_path)
        return
    for filename in os.listdir(folder_path):
        if filename.lower().endswith(".pdf"):
            try:
                with open(os.path.join(folder_path, filename), "rb") as f:
                    pdf = PyPDF2.PdfReader(f)
                    content = ""
                    for page in pdf.pages:
                        content += page.extract_text() + "\n"
                    if content.strip():
                        # Chia nh·ªè vƒÉn b·∫£n ƒë·ªÉ t√¨m ki·∫øm ch√≠nh x√°c h∆°n
                        chunks = [content[i:i+800] for i in range(0, len(content), 600)]
                        for chunk in chunks:
                            _policy_database.append({"source": filename, "content": chunk.strip()})
                print(f"[PDF] ‚úÖ ƒê√£ n·∫°p file: {filename}")
            except Exception as e:
                print(f"[PDF] ‚ùå L·ªói ƒë·ªçc file {filename}: {e}")
    if _policy_database:
        print(f"[PDF] ‚öôÔ∏è ƒêang t·∫°o vector cho {len(_policy_database)} ƒëo·∫°n ch√≠nh s√°ch...")
        _policy_embeddings_cache = [generate_embedding(item["content"]) for item in _policy_database]

def search_policies_vector(query: str, top_k: int = 2):
    """T√¨m ki·∫øm ng·ªØ nghƒ©a trong d·ªØ li·ªáu PDF ch√≠nh s√°ch"""
    if not _policy_embeddings_cache: return []
    query_emb = generate_embedding(query)
    scores = []
    for i, p_emb in enumerate(_policy_embeddings_cache):
        sim = cosine_similarity(query_emb, p_emb)
        scores.append((i, sim))
    
    scores.sort(key=lambda x: x[1], reverse=True)
    results = [_policy_database[i] for i, sim in scores[:top_k] if sim > 0.2]
    
    # --- 2. Keyword Fallback (N·∫øu Vector Search th·∫•t b·∫°i) ---
    if not results:
        safe_print(f"‚ö†Ô∏è [PDF] Vector search th·∫•p, th·ª≠ t√¨m b·∫±ng t·ª´ kh√≥a cho: {query}")
        keywords = [k for k in query.lower().split() if len(k) > 2]
        for item in _policy_database:
            content_lower = item["content"].lower()
            # N·∫øu ƒëo·∫°n vƒÉn b·∫£n ch·ª©a b·∫•t k·ª≥ t·ª´ kh√≥a quan tr·ªçng n√†o (v√†o n∆∞·ªõc, b·∫£o h√†nh...)
            if any(k in content_lower for k in keywords):
                results.append(item)
                if len(results) >= top_k: break
                
    return results

async def retrieve_context(
    user_message: str,
    backend_url: str,
    use_vector_search: bool = True,
    use_llm_reranking: bool = False
) -> Dict:
    """
    MAIN ISSUE: Vector search may not work, falling back to keyword search
    which might return same products due to backend API limitations
    """
    """
    Retrieve context t·ª´ nhi·ªÅu ngu·ªìn: products, reviews, FAQs.
    
    Args:
        user_message: C√¢u h·ªèi c·ªßa user
        backend_url: URL c·ªßa backend API
        use_vector_search: N·∫øu True, d√πng Vector Search (semantic). N·∫øu False, d√πng Keyword Search (fallback)
        use_llm_reranking: N·∫øu True, d√πng th√™m LLM ƒë·ªÉ rerank k·∫øt qu·∫£ vector search (optional)
    
    Strategy (Vector Search):
        1. Vector Search: L·∫•y products t·ª´ backend ‚Üí t·∫°o embeddings ‚Üí similarity search
        2. Optional LLM Reranking: Fine-tune ranking n·∫øu c·∫ßn
        3. Multi-source: K·∫øt h·ª£p products + reviews + FAQs
    """
    try:
        print(f"üîç [RAG] Starting retrieval for: {user_message}")
        
        vector_results = []
        final_products = []
        search_term_used = ""
        price_condition, price_value = extract_price_intent(user_message)
        
        # 1. Kh·ªüi t·∫°o bi·∫øn ƒë·ªÉ tr√°nh l·ªói UnboundLocalError
        relevant_policies = []

        # 2. S·ª≠ d·ª•ng ƒë√∫ng h√†m check ch√≠nh s√°ch (b·∫°n ƒë√£ ƒë·ªãnh nghƒ©a nh∆∞ng ch∆∞a d√πng)
        if should_search_policies(user_message):
         relevant_policies = search_policies_vector(user_message)
        print(f"[PDF] Found {len(relevant_policies)} policy chunks")

        # --- B∆Ø·ªöC 2: T√¨m ki·∫øm S·∫£n ph·∫©m t·ª´ Database ---
        if should_search_products(user_message):
            if use_vector_search:
                # ===== VECTOR SEARCH (Semantic Search) =====
                print("üî¢ [RAG] Using Vector Search (Semantic Search)")
                
                try:
                    # 1. L·∫•y products t·ª´ backend (kh√¥ng c·∫ßn search_term)
                    all_products = await get_products_from_backend(backend_url, limit=50)
                    print(f"üì¶ [RAG] Fetched {len(all_products)} products from backend")
                    
                    if all_products:
                        # 1.5. Pre-filter theo gi√° tr∆∞·ªõc khi vector search ƒë·ªÉ tr√°nh l·ªách gi√°
                        all_products = prefilter_products_by_price(all_products, price_condition, price_value)

                        # 2. Vector similarity search
                        try:
                            vector_results = await vector_search_products(
                                user_message, 
                                all_products, 
                                top_k=10
                            )
                            
                            # Extract products t·ª´ results (b·ªè similarity scores)
                            # Ch·ªâ l·∫•y s·∫£n ph·∫©m c√≥ similarity > threshold (0.3) ƒë·ªÉ ƒë·∫£m b·∫£o li√™n quan
                            SIMILARITY_THRESHOLD = 0.3
                            final_products = [
                                product for product, score in vector_results 
                                if score >= SIMILARITY_THRESHOLD
                            ]
                            
                            if not final_products and vector_results:
                                # N·∫øu kh√¥ng c√≥ s·∫£n ph·∫©m n√†o ƒë·∫°t threshold, l·∫•y top 3 c√≥ similarity cao nh·∫•t
                                print(f"‚ö†Ô∏è [RAG] No products above threshold {SIMILARITY_THRESHOLD}, using top 3")
                                final_products = [product for product, score in vector_results[:3]]
                            
                            print(f"üìä [RAG] Filtered to {len(final_products)} products above threshold")
                            
                            # Optional: LLM reranking ƒë·ªÉ fine-tune
                            if use_llm_reranking and final_products:
                                print("üß† [RAG] Applying LLM reranking...")
                                final_products = await semantic_search(user_message, final_products)
                                print(f"üß† [RAG] LLM reranking completed")
                            
                            print(f"‚úÖ [RAG] Vector search found {len(final_products)} relevant products")
                        except Exception as vec_error:
                            # Vector search failed (model ch∆∞a load, ho·∫∑c l·ªói kh√°c)
                            print(f"‚ö†Ô∏è [RAG] Vector search failed: {vec_error}, falling back to keyword search")
                            use_vector_search = False  # Trigger fallback
                            raise  # Re-raise ƒë·ªÉ trigger fallback block
                    else:
                        print("‚ö†Ô∏è [RAG] No products from backend, skipping vector search")
                        use_vector_search = False  # Fallback to keyword
                except Exception as e:
                    # Vector search failed, fallback to keyword search
                    print(f"‚ö†Ô∏è [RAG] Vector search error: {e}, falling back to keyword search")
                    use_vector_search = False
            
            # N·∫øu vector search th√†nh c√¥ng nh∆∞ng kh√¥ng ra s·∫£n ph·∫©m, fallback keyword search
            if use_vector_search and not final_products:
                print("üîÑ [RAG] No products from vector search, fallback to keyword search")
                search_term_used = extract_search_term(user_message)
                keyword_results = await get_products_from_backend(backend_url, search_term_used)
                keyword_results = prefilter_products_by_price(keyword_results, price_condition, price_value)
                print(f"üì¶ [RAG] Keyword fallback found: {len(keyword_results)} products")
                final_products = keyword_results

            if not use_vector_search:
                # ===== KEYWORD SEARCH (Fallback) =====
                print("üîë [RAG] Using Keyword Search (fallback)")
                search_term_used = extract_search_term(user_message)
                keyword_results = await get_products_from_backend(backend_url, search_term_used)
                keyword_results = prefilter_products_by_price(keyword_results, price_condition, price_value)
                print(f"üì¶ [RAG] Keyword search found: {len(keyword_results)} products")
                final_products = keyword_results
        
        # Reviews v√† FAQs v·∫´n d√πng keyword-based (c√≥ th·ªÉ upgrade sau)
        keywords = extract_keywords(user_message)
        reviews = await get_reviews_from_backend(backend_url, keywords)
        faqs = get_faqs(user_message)
        

        context = {
            "products": final_products,
            "reviews": reviews,
            "faqs": faqs,
            "policies": relevant_policies,  # Th√™m k·∫øt qu·∫£ PDF v√†o context
            "query": user_message,
            "search_term": search_term_used
        }
        
        print(f"‚úÖ [RAG] Retrieved: {len(context['products'])} products, {len(context['reviews'])} reviews, {len(context['faqs'])} FAQs")
        
        return context
    except Exception as e:
        print(f"[RAG] Error in retrieval: {e}")
        import traceback
        traceback.print_exc()
        return {
            "products": [],
            "reviews": [],
            "faqs": [],
            "query": user_message,
            "search_term": ""
        }

def format_rag_context(context: Dict) -> str:
    formatted_context = ""
    
     # Th√™m ph·∫ßn Ch√≠nh s√°ch t·ª´ PDF v√†o format ng·ªØ c·∫£nh
    if context.get("policies"):
        formatted_context += "\n\n[QUY ƒê·ªäNH C·ª¨A H√ÄNG T·ª™ PDF]:\n"
        for p in context["policies"]:
            formatted_context += f"- {p['content']}\n"
        formatted_context += "‚ö†Ô∏è L∆ØU √ù: Tr·∫£ l·ªùi kh√°ch ƒë√∫ng theo quy ƒë·ªãnh n√†y.\n"

    if context.get("products"):
        formatted_context += "\n\n[TH√îNG TIN S·∫¢N PH·∫®M T·ª™ DATABASE - D·ªÆ LI·ªÜU TH·ª∞C T·∫æ]:\n"
        formatted_context += "ƒê√¢y l√† danh s√°ch s·∫£n ph·∫©m ƒë∆∞·ª£c t√¨m th·∫•y (ƒë√£ ƒë∆∞·ª£c x·∫øp h·∫°ng theo m·ª©c ƒë·ªô li√™n quan):\n\n"
        
        for idx, product in enumerate(context["products"][:5], 1):
            formatted_context += f"{idx}. {product['name']}\n"
            formatted_context += f"   - Danh m·ª•c: {product.get('category', 'N/A')}\n"
            price = product.get('price', 0)
            formatted_context += f"   - Gi√°: {price:,} VNƒê ({price/1000000:.1f} tri·ªáu ƒë·ªìng)\n"
            stock = product.get('stockQuantity', 0)
            formatted_context += f"   - T·ªìn kho: {'C√≤n ' + str(stock) + ' s·∫£n ph·∫©m' if stock > 0 else 'H·∫øt h√†ng'}\n"
            desc = product.get('description', '')
            if desc:
                desc_short = desc[:150] + "..." if len(desc) > 150 else desc
                formatted_context += f"   - M√¥ t·∫£: {desc_short}\n"
            formatted_context += f"   - Product ID: {product.get('productId', 'N/A')}\n\n"
        
        formatted_context += "‚ö†Ô∏è QUAN TR·ªåNG: B·∫°n PH·∫¢I s·ª≠ d·ª•ng CH√çNH X√ÅC th√¥ng tin tr√™n ƒë·ªÉ tr·∫£ l·ªùi. "
        formatted_context += "ƒê√¢y l√† d·ªØ li·ªáu TH·ª∞C T·∫æ t·ª´ database. "
        formatted_context += "N·∫øu kh√°ch h·ªèi v·ªÅ gi√°, h√£y L·ªåC v√† LI·ªÜT K√ä c√°c s·∫£n ph·∫©m ph√π h·ª£p.\n"
    
    if context.get("reviews"):
        formatted_context += "\n\n[ƒê√ÅNH GI√Å T·ª™ KH√ÅCH H√ÄNG]:\n"
        for idx, review in enumerate(context["reviews"][:3], 1):
            product_name = review.get('product', {}).get('name', 'N/A')
            rating = review.get('rating', 0)
            comment = review.get('comment', '')
            formatted_context += f"{idx}. {product_name}: {rating}/5 sao\n"
            formatted_context += f'   "{comment}"\n\n'
    
    if context.get("faqs"):
        formatted_context += "\n\n[TH√îNG TIN H·ªñ TR·ª¢]:\n"
        for faq in context["faqs"]:
            formatted_context += f"Q: {faq['question']}\n"
            formatted_context += f"A: {faq['answer']}\n\n"
    
    return formatted_context

# G·ªçi n·∫°p PDF ngay khi kh·ªüi ƒë·ªông app
load_policies_from_pdfs()


# rag_service.py

# ... (c√°c import c≈© gi·ªØ nguy√™n)

async def identify_phone_from_image(image_bytes: bytes) -> str:
    """
    S·ª≠ d·ª•ng Gemini Vision ƒë·ªÉ nh·∫≠n di·ªán t√™n ƒëi·ªán tho·∫°i t·ª´ h√¨nh ·∫£nh.
    C√≥ c∆° ch·∫ø t·ª± ƒë·ªông th·ª≠ model kh√°c n·∫øu model m·∫∑c ƒë·ªãnh l·ªói.
    """
    print("[VISION] Analyzing image...")
    
    # Danh s√°ch c√°c model vision ƒë·ªÉ th·ª≠ l·∫ßn l∆∞·ª£t (∆∞u ti√™n Flash v√¨ nhanh/r·∫ª)
    candidate_models = [
        'gemini-2.5-flash', 
        'gemini-2.0-flash-lite' 
        
    ]

    image = None
    try:
        image = PIL.Image.open(io.BytesIO(image_bytes))
    except Exception as e:
        print(f"[VISION] L·ªói ƒë·ªçc ·∫£nh: {e}")
        return ""

    prompt = """
    H√£y nh√¨n v√†o h√¨nh ·∫£nh n√†y v√† x√°c ƒë·ªãnh ch√≠nh x√°c ƒë√¢y l√† ƒëi·ªán tho·∫°i g√¨.
    Ch·ªâ c·∫ßn n√≥i t√™n ƒëi·ªán tho·∫°i (H√£ng + Model + M√†u).
    V√≠ d·ª•: "iPhone 15 Pro Max Titanium".
    Kh√¥ng gi·∫£i th√≠ch th√™m.
    """

    for model_name in candidate_models:
        try:
            print(f"[VISION] Trying model: {model_name}...")
            model = genai.GenerativeModel(model_name)
            response = model.generate_content([prompt, image])
            
            if response and response.text:
                result = response.text.strip()
                print(f"[VISION] Success with {model_name}: {result}")
                return result
                
        except Exception as e:
            # N·∫øu l·ªói "Not Found" ho·∫∑c l·ªói kh√°c, th·ª≠ model ti·∫øp theo
            print(f"[VISION] Failed with {model_name}: {str(e)}")
            continue

    print("[VISION] All models failed to analyze the image.")
    return ""
